{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AWS Glue: Reading from & Writing to S3 ☁️\n",
                "\n",
                "This notebook performs a **Real-World ETL Job**:\n",
                "1. **EXTRACT**: Read CSV data directly from your S3 bucket.\n",
                "2. **TRANSFORM**: Clean/modify the schema.\n",
                "3. **LOAD**: Write the results back to S3 as Parquet.\n",
                "\n",
                "*Prerequisite: Ensure `sales_data.csv` is uploaded to `s3://egirgis-datalake-v1/raw/sales_data/`*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pyspark.context import SparkContext\n",
                "from awsglue.context import GlueContext\n",
                "from awsglue.dynamicframe import DynamicFrame\n",
                "from awsglue.utils import getResolvedOptions\n",
                "\n",
                "# 1. Setup Context\n",
                "sc = SparkContext.getOrCreate()\n",
                "glueContext = GlueContext(sc)\n",
                "spark = glueContext.spark_session\n",
                "print(\"Glue Context Initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Read from S3 (Extract)\n",
                "We verify we can reach the bucket and read the file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define S3 Paths (Change these if you use a different bucket)\n",
                "BUCKET_NAME = \"egirgis-datalake-v1\"\n",
                "INPUT_PATH = f\"s3://{BUCKET_NAME}/raw/sales_data/\"\n",
                "OUTPUT_PATH = f\"s3://{BUCKET_NAME}/processed/sales_clean/\"\n",
                "\n",
                "print(f\"Reading from: {INPUT_PATH}\")\n",
                "\n",
                "# Read CSV from S3\n",
                "dyf_s3 = glueContext.create_dynamic_frame.from_options(\n",
                "    format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\"},\n",
                "    connection_type=\"s3\",\n",
                "    format=\"csv\",\n",
                "    connection_options={\"paths\": [INPUT_PATH], \"recurse\": True},\n",
                "    transformation_ctx=\"input_dyf\"\n",
                ")\n",
                "\n",
                "dyf_s3.show(5)\n",
                "dyf_s3.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Transform\n",
                "Let's fix types (Sales to double) and rename columns exactly as we would in a real job."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mapped_dyf = dyf_s3.apply_mapping([\n",
                "    (\"City\", \"string\", \"city\", \"string\"),\n",
                "    (\"Product\", \"string\", \"product_name\", \"string\"),\n",
                "    (\"Sales\", \"string\", \"sales_amount\", \"double\"),\n",
                "    (\"Date\", \"string\", \"date\", \"string\")\n",
                "])\n",
                "\n",
                "mapped_dyf.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Write to S3 (Load)\n",
                "Write the clean data back to S3 in Parquet format (best for analytics)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Writing to: {OUTPUT_PATH}\")\n",
                "\n",
                "glueContext.write_dynamic_frame.from_options(\n",
                "    frame=mapped_dyf,\n",
                "    connection_type=\"s3\",\n",
                "    format=\"parquet\",\n",
                "    connection_options={\"path\": OUTPUT_PATH, \"partitionKeys\": [\"city\"]},\n",
                "    transformation_ctx=\"output_dyf\"\n",
                ")\n",
                "\n",
                "print(\"Write Complete! Check your S3 bucket for the 'processed' folder.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}