{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ‘½ Project: Alien Insights ETL Pipeline\n",
                "\n",
                "**Goal**: Clean and optimize raw UFO sightings data for analysis.\n",
                "\n",
                "### Pipeline Steps:\n",
                "1.  **Extract**: Read `ufo_sightings.csv` from S3 Raw.\n",
                "2.  **Clean (Silver Layer)**:\n",
                "    *   Standardize column names (remove spaces).\n",
                "    *   Handle missing values in `State` and `Shape`.\n",
                "    *   **Challenge**: Convert messy `Time` string to actual `Timestamp`.\n",
                "3.  **Feature Engineering (Gold Layer)**:\n",
                "    *   Extract `Year` and `Month` for trend analysis.\n",
                "    *   Aggregate total sightings per `State`.\n",
                "4.  **Load**: Write optimized Parquet data back to S3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pyspark.context import SparkContext\n",
                "from awsglue.context import GlueContext\n",
                "from awsglue.dynamicframe import DynamicFrame\n",
                "from pyspark.sql.functions import col, to_timestamp, year, month, count, trim, upper, when\n",
                "\n",
                "# 1. Setup\n",
                "sc = SparkContext.getOrCreate()\n",
                "glueContext = GlueContext(sc)\n",
                "spark = glueContext.spark_session\n",
                "print(\"Ready to find aliens! ðŸ›¸\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Extract (Read from S3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BUCKET_NAME = \"egirgis-datalake-v1\"\n",
                "INPUT_PATH = f\"s3://{BUCKET_NAME}/raw/ufo/\"\n",
                "OUTPUT_PATH_GOLD = f\"s3://{BUCKET_NAME}/gold/ufo_analytics/\"\n",
                "\n",
                "# Read Raw CSV\n",
                "dyf_raw = glueContext.create_dynamic_frame.from_options(\n",
                "    format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\"},\n",
                "    connection_type=\"s3\",\n",
                "    format=\"csv\",\n",
                "    connection_options={\"paths\": [INPUT_PATH], \"recurse\": True},\n",
                "    transformation_ctx=\"raw_input\"\n",
                ")\n",
                "\n",
                "print(\"Raw Data Schema:\")\n",
                "dyf_raw.printSchema()\n",
                "dyf_raw.show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Clean & Normalize (Silver Layer)\n",
                "We convert to Spark DataFrame for easier column manipulation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_raw = dyf_raw.toDF()\n",
                "\n",
                "# 1. Rename columns (Remove spaces)\n",
                "df_cleaned = df_raw.withColumnRenamed(\"Colors Reported\", \"Colors_Reported\") \\\n",
                "                   .withColumnRenamed(\"Shape Reported\", \"Shape\")\n",
                "\n",
                "# 2. Clean 'Shape': Trim whitespace, uppercase, replace NULL with 'UNKNOWN'\n",
                "df_cleaned = df_cleaned.withColumn(\"Shape\", upper(trim(col(\"Shape\")))) \\\n",
                "                       .fillna({\"Shape\": \"UNKNOWN\"})\n",
                "\n",
                "# 3. Parse Time (Format is like '6/1/1930 22:00')\n",
                "# Spark's to_timestamp handles standard formats well, but let's be explicit\n",
                "df_cleaned = df_cleaned.withColumn(\"Event_Time\", to_timestamp(col(\"Time\"), \"M/d/yyyy H:mm\"))\n",
                "\n",
                "df_cleaned.printSchema()\n",
                "df_cleaned.select(\"City\", \"State\", \"Shape\", \"Event_Time\").show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Feature Engineering (Gold Layer)\n",
                "Extract insights: Year, Month, and aggregations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract Time Features\n",
                "df_gold = df_cleaned.withColumn(\"Year\", year(col(\"Event_Time\"))) \\\n",
                "                    .withColumn(\"Month\", month(col(\"Event_Time\")))\n",
                "\n",
                "# Filter out bad dates (if parsing failed)\n",
                "df_gold = df_gold.filter(col(\"Year\").isNotNull())\n",
                "\n",
                "print(f\"Total Cleaned Sightings: {df_gold.count()}\")\n",
                "df_gold.show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Load (Write to S3)\n",
                "Write the final dataset partitioned by **Year** (efficient for time-series queries)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert back to DynamicFrame for Glue Writing (optional, but good practice)\n",
                "dyf_gold = DynamicFrame.fromDF(df_gold, glueContext, \"gold_output\")\n",
                "\n",
                "print(f\"Writing Gold Data to: {OUTPUT_PATH_GOLD}\")\n",
                "\n",
                "# Write Parquet, Partitioned by Year\n",
                "glueContext.write_dynamic_frame.from_options(\n",
                "    frame=dyf_gold,\n",
                "    connection_type=\"s3\",\n",
                "    format=\"parquet\",\n",
                "    connection_options={\"path\": OUTPUT_PATH_GOLD, \"partitionKeys\": [\"Year\"]},\n",
                "    transformation_ctx=\"write_gold\"\n",
                ")\n",
                "\n",
                "print(\"mission_complete = True ðŸ›¸\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}