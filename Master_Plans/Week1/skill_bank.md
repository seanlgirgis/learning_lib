# üè¶ The Skills Bank: Data Engineering Translation Layer

**Purpose:** Master inventory of your 20-year career, reframed for **Senior Data Engineer** roles.
**Strategy:** "Performance Engineering" = "Data Engineering on Telemetry Data".

---

## üèõÔ∏è **CITI (2017 - 2025)**
*Role: Senior Capacity Consultant ‚Üí **Senior Data Engineer (Telemetry Pipelines)***

| Original Duty (Source) | Mapped DE Term (Target) | Key Skills to Highlight |
| :--- | :--- | :--- |
| "Architected big-data forecasting for 10,000+ global servers." | **Designed Enterprise Data Lake**: Architected the forecasting data lake ingestion layer for 10k+ nodes. | Data Architecture, Scalability, System Design |
| "Built automated ETL pipelines to extract and process metrics from BMC TrueSight and AppDynamics." | **Built Telemetry ETL Pipelines**: Engineered Python/Pandas pipelines to ingest real-time timeseries data from APM sources. | Python, ETL, API Integration, Timeseries DB |
| "Developed ML models using Python and scikit-learn to predict infrastructure requirements." | **MLOps & Predictive Analytics**: Deployed production Prophet/Scikit-learn models for capacity forecasting. | MLOps, Scikit-learn, Prophet, Forecasting |
| "Automated regional forecasting, reducing manual analysis by 85%." | **Pipeline Automation**: Orchestrated automated workflows to replace manual excel-based reporting processes. | Workflow Orchestration, Automation, Efficiency |
| "Fixed 'small file' issues by implementing Snappy Parquet compression." | **Big Data Optimization**: Optimized S3 storage layout using Parquet partitioning and Snappy compression. | Parquet, S3 Optimization, File Formats |
| "Built a 'Text-to-SQL' GenAI bot using Claude 3 Sonnet." | **AI-Augmented Data Access**: Developed GenAI agents (RAG) to democratize data access via natural language. | GenAI, LLMs, RAG, LangChain |

---

## üè® **G6 HOSPITALITY (2017)**
*Role: Performance Engineer ‚Üí **Data Reliability Engineer***

| Original Duty (Source) | Mapped DE Term (Target) | Key Skills to Highlight |
| :--- | :--- | :--- |
| "Led the 'FAST' project, data mining user performance metrics." | **Log Analysis & Mining**: Built log-mining pipelines to extract user behavior signals from raw server logs. | Log Parsing, Regex, Data Mining, Unstructured Data |
| "Implemented custom dashboards, alerts, and reports." | **Real-Time Analytics Dashboarding**: Designed operational dashboards for business-critical KPIs. | Dashboarding, Visualization, Monitoring |
| "Managed end-to-end monitoring for Brand.com using Dynatrace." | **Observability Pipeline**: Managed full-stack observability data flow (APM + Synthetics). | Observability, Dynatrace, Telemetry |

---

## ‚òÅÔ∏è **CA TECHNOLOGIES / TIAA-CREF (2011 - 2017)**
*Role: Senior Consultant ‚Üí **Platform Engineer (Data Infrastructure)***

| Original Duty (Source) | Mapped DE Term (Target) | Key Skills to Highlight |
| :--- | :--- | :--- |
| "Managed 4,000‚Äì6,000 agents across multi-cluster environments." | **High-Scale Distributed Systems**: Managed distributed data collection agents across 4k+ nodes. | Distributed Systems, Scalability, Cluster Mgmt |
| "Designed Perl/Ksh data-extraction scripts for business-specific insights." | **Script-Based ETL**: Developed custom ETL scripts (Perl/Shell) for proprietary data extraction. | Scripting (Bash/Perl), Automation, Legacy ETL |
| "Resolving complex issues/clamps in high-stakes environments." | **Pipeline Troubleshooting**: Root cause analysis for data ingestion bottlenecks (clamps) in high-throughput systems. | Troubleshooting, Performance Tuning, RCA |

---

## ‚úàÔ∏è **SABRE (2008 - 2010)**
*Role: Senior Software Engineer (C++) ‚Üí **Backend Data Engineer***

| Original Duty (Source) | Mapped DE Term (Target) | Key Skills to Highlight |
| :--- | :--- | :--- |
| "High-performance conversion of Shopping Engine from MySQL to Oracle." | **Database Migration & Optimization**: Led zero-downtime database migration for high-transactionOLTP systems. | Database Migration, SQL Tuning, Oracle, MySQL |
| "Converted SQL queries, achieved superior DSS performance." | **SQL Query Optimization**: Refactored complex SQL queries for 10x performance gain in Decision Support Systems. | SQL Optimization, Query Planning, DSS |
| "Reducing server footprint dramatically." | **Resource Optimization**: Optimized compute/storage utilization for cost-efficiency. | Compute Optimization, Efficiency |

---

## üîå **EARLY LEGACY (Sprint/Corpus) (2001 - 2008)**
*Role: Developer ‚Üí **Mainframe/Legacy Data Engineer***

| Original Duty (Source) | Mapped DE Term (Target) | Key Skills to Highlight |
| :--- | :--- | :--- |
| "Performance enhancements in billing processes... reducing memory usage 75%." | **Batch Processing Optimization**: Tuned large-scale batch processing jobs for memory efficiency. | C++, Memory Management, Batch Processing |
| "Worked on CICS/MQSeries/XML messaging architecture." | **Message Queue Integration**: Integrated real-time message streams (MQSeries) for asynchronous data flow. | Messaging Queues (MQ), XML, Event-Driven Arch |
| "Automated system administration... with Korn Shell scripts." | **Infrastructure Automation**: Built shell-based automation for system maintenance. | Shell Scripting, Linux Admin |

---

## üõ†Ô∏è **Summary of Transferable Skills**
*   **Performance Engineering** is just **Data Engineering** focused on *system metrics data*.
*   **Capacity Planning** is just **Predictive Analytics**.
*   **APM Tools** are **Data Ingestion Engines**.
*   **Root Cause Analysis** is **Data Debugging**.


# Skills & Duties Bank ‚Äì Sean Luka Girgis
## Overview
Centralized inventory of 25+ years experience, translated to modern Data Engineering, ML, and Performance terms.  
Pull from this for resumes, CV, LinkedIn, website updates.  
Columns:  
- **Original Context** (job/company/dates + original bullet)  
- **DE/ML Mapping** (modern translation)  
- **Key Skills/Tools**  
- **Quantifiable Impact** (if available)  
- **Suggested Resume Bullet** (targeted for Senior Data Engineer / ML Engineer roles)

## Citi Financial ‚Äì Performance & Capacity Consultant (Nov 2017 ‚Äì Dec 2025)
| Original Context | DE/ML Mapping | Key Skills/Tools | Quantifiable Impact | Suggested Resume Bullet |
|------------------|---------------|------------------|---------------------|-------------------------|
| Data Mining and Creating Capacity reports | Telemetry Data Pipeline + ETL for Capacity Metrics | Python, pandas, ETL, data mining, CA APM, BMC TrueSight | Monthly executive reports | Engineered Python/pandas ETL pipelines to extract, transform, and mine capacity metrics from enterprise APM tools (CA APM 10.5, BMC TrueSight), enabling automated monthly capacity reporting and trend analysis |
| Built automated data pipelines using Python and pandas... | Automated Data Ingestion & Transformation Pipeline | Python, pandas, ETL automation, data integration | Reduced manual effort by X% | Developed automated Python/pandas pipelines consolidating monitoring data from multiple sources into a unified analytics database, improving data accuracy and stakeholder visibility |
| Developed machine learning forecasting models using Python and scikit-learn | Time-Series Forecasting Pipeline + ML Feature Engineering | scikit-learn, time-series, ML pipelines, Prophet/XGBoost/LSTM (from Horizon) | Improved forecast accuracy | Implemented scikit-learn (and advanced models like Prophet/XGBoost) time-series forecasting for 3-6 month capacity predictions across 10,000+ servers, reducing emergency provisioning incidents |
| Managed CA APM environments... On Boarding of new application... Creating and Maintaining Data Mining Scripts | Monitoring Data Pipeline Management + Custom ETL Scripts | APM tools (CA APM), scripting (Python/Perl), onboarding pipelines | Supported 6000+ agents in prior roles | Administered high-volume APM telemetry pipelines, onboarded new applications, and built custom Python data mining scripts for scalable capacity analytics |

## HorizonScale Project (Flagship ML Showcase ‚Äì Ongoing)
| Original Context | DE/ML Mapping | Key Skills/Tools | Quantifiable Impact | Suggested Resume Bullet |
|------------------|---------------|------------------|---------------------|-------------------------|
| HorizonScale AI predictive utilization pipeline | End-to-End Scalable ML Forecasting Pipeline | Python, Prophet, XGBoost, LSTM, multiprocessing (Turbo mode), Streamlit, synthetic data, Parquet | 90% reduction in forecasting cycles via parallel processing | Architected HorizonScale: high-performance time-series forecasting system using automated model selection (Prophet/XGBoost/LSTM), multiprocessing for 10x throughput, and Streamlit dashboard for 6-month capacity outlooks on 2,000+ nodes |

## Next Roles to Add (Prioritize This Week)
- G6 Hospitality (DynaTrace monitoring + AWS migration ‚Üí telemetry ingestion + cloud ETL)
- Sabre (MySQL to Oracle conversion + C++/OCCI ‚Üí batch data migration + optimization)
- Sprint / Corpus Inc. (C++/Pro*C batch processing + performance enhancements ‚Üí efficient ETL pipelines)