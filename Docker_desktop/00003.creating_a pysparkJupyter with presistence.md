To ensure your work is saved, always store your notebooks and datasets inside the **`/home/jovyan/work`** directory within Jupyter, as this is the only folder linked to your persistent volume.

---

## ðŸš€ Summary: Deploying Persistent PySpark on Docker Desktop

This guide summarizes how we set up a functional, data-persistent Spark environment using Docker Desktop and the Jupyter Docker Stacks.

### 1. Key Concepts Learned

* **Images:** The blueprint (template) for your environment (`jupyter/pyspark-notebook`).
* **Containers:** The running instance of that image (the "living" environment).
* **Volumes:** A storage area managed by Docker that exists independently of the container.
* **Port Mapping:** Connecting the container's internal port (`8888`) to your Windows machine (`localhost:8888`).

### 2. The Setup Workflow

| Step | Action | Command / Logic |
| --- | --- | --- |
| **1. Create Volume** | Create a permanent storage space. | `docker volume create pyspark_data` |
| **2. Pull Image** | Download the PySpark environment. | `docker pull jupyter/pyspark-notebook` |
| **3. Run Container** | Launch and link volume/ports. | See Command Below |
| **4. Access** | Find the login token. | Found in Docker Desktop **Logs** tab. |

### 3. The Master Execution Command

This is the command used to tie everything together:

```bash
docker run -d -p 8888:8888 --name my-pyspark-lab -v pyspark_data:/home/jovyan/work jupyter/pyspark-notebook

docker run -d \
  -p 8888:8888 \
  --name my-pyspark-lab \
  -v pyspark_data:/home/jovyan/work \
  jupyter/pyspark-notebook

```

# Path of the volumes
```bash
\\wsl.localhost\docker-desktop\mnt\docker-desktop-disk\data\docker\volumes\pyspark_data
```

* **`-d`**: Runs in the background (Detached).
* **`-p 8888:8888`**: Lets you access Jupyter at `http://localhost:8888`.
* **`--name`**: Gives the container a friendly name in the GUI.
* **`-v`**: Maps your named volume (`pyspark_data`) to the internal Jupyter work folder.

### 4. Verification & Testing

* **Persistence Test:** We created a file in the `work` folder, deleted the container, and re-launched a new one. The file remained because it was stored in the **Volume**, not the container's temporary memory.
* **Spark Test:** We initialized a `SparkSession` to confirm that the Java and Spark dependencies pre-installed in the image are functional.

---

### Helpful Tips for Future Use

* **To Stop:** Use the Stop button in the Docker Desktop GUI or run `docker stop my-pyspark-lab`.
* **To Start Again:** Use the Play button in the GUI or run `docker start my-pyspark-lab`.
* **New Libraries:** If you need to install a library (like `pandas` or `scikit-learn`), you can open a terminal inside Jupyter and run `%pip install library-name`.

**Would you like me to show you how to connect a local folder on your Windows Desktop (a Bind Mount) so you can drag-and-drop files directly from Windows into Spark?**