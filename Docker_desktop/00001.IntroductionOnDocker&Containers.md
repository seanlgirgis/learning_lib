### Introduction to Docker and Containers

Docker is a platform that allows you to package, distribute, and run applications in isolated environments called **containers**. Containers are lightweight, portable, and consistent across different machines, making them ideal for development, testing, and deployment. Unlike virtual machines (VMs), which emulate entire operating systems, containers share the host OS kernel but isolate processes, filesystems, and networks.

Key benefits of containers:
- **Consistency**: Your app runs the same way on your laptop as it does in production.
- **Isolation**: Each container has its own environment, dependencies, and resources.
- **Efficiency**: Containers start quickly and use fewer resources than VMs.
- **Portability**: Build once, run anywhere (as long as Docker is installed).

Docker Desktop is the user-friendly GUI and CLI tool for running Docker on your local machine (Windows, macOS, or Linux). It includes the Docker Engine, CLI, Compose, and additional features like a dashboard for managing containers. Since you have the latest version (as of January 2026, that's likely Docker Desktop 4.XX or higher—check via the app's "About" section), it supports modern features like enhanced security, WSL 2 integration on Windows, and extensions for plugins.

### Getting Started with Docker Desktop

1. **Installation and Verification**:
   - You've already installed it, but to verify: Open Docker Desktop. It should show a whale icon in your system tray/menu bar.
   - Run `docker --version` in your terminal (Command Prompt, PowerShell, or Terminal) to confirm. It should output something like "Docker version 27.X.X".
   - If it's not running, start it from the app. Docker Desktop runs a lightweight VM in the background to host the Docker Engine.

2. **The Docker Desktop Dashboard**:
   - **Containers Tab**: Lists running and stopped containers. You can start/stop/restart, view logs, inspect details, or exec into a shell (e.g., `docker exec -it <container-name> bash`).
   - **Images Tab**: Shows downloaded images (blueprints for containers). Pull new ones with `docker pull <image-name>`.
   - **Volumes Tab**: Manages persistent storage. Volumes are Docker-managed directories that survive container deletion.
   - **Dev Environments Tab**: For setting up development environments with Git integration.
   - **Extensions Tab**: This is likely what you mean by "new MCP menu and plugins." "MCP" might refer to the Marketplace for Extensions (Docker's plugin system). Here, you can browse and install extensions like:
     - Portainer for advanced container management.
     - Disk usage analyzers.
     - Kubernetes tools (if enabled).
     - Database viewers (e.g., for PostgreSQL or Redis).
     To use: Click "Extensions" > "Marketplace" > Search and install. Once installed, they appear in the left sidebar for quick access. Extensions extend Docker's capabilities without messing with core setup—great for adding tools like monitoring or CI/CD integrations.

3. **Settings and Configuration**:
   - Go to the gear icon (Settings).
   - **General**: Enable auto-start, theme, etc.
   - **Resources**: Adjust CPU, memory, and disk allocation for the Docker VM.
   - **Docker Engine**: Edit the daemon config (JSON) for advanced tweaks like insecure registries.
   - **Kubernetes**: Enable if you want to run K8s locally (single-node cluster).
   - **Experimental Features**: Toggle betas like enhanced container isolation.

### Working with Containers

Containers are instances of images. An image is like a template (e.g., the PySpark Jupyter image you mentioned), and a container is a running or stopped instance of it.

1. **Basic Commands (Use CLI for Proficiency)**:
   - **Pull an Image**: `docker pull <image-name:tag>` (e.g., `docker pull jupyter/pyspark-notebook:latest` for a PySpark Jupyter setup).
   - **Run a Container**: `docker run -d -p 8888:8888 --name my-notebook jupyter/pyspark-notebook`. 
     - `-d`: Detached (background).
     - `-p`: Port mapping (host:container).
     - `--name`: Custom name.
   - **List Containers**: `docker ps` (running) or `docker ps -a` (all).
   - **Stop/Start**: `docker stop <name/id>`, `docker start <name/id>`.
   - **Remove**: `docker rm <name/id>` (stopped containers), `docker rmi <image>` (images).
   - **Logs**: `docker logs <name/id>`.
   - **Exec into Container**: `docker exec -it <name/id> bash` to open a shell inside.

2. **Capabilities of Containers**:
   - **Networking**: Containers can communicate via networks. Create custom ones with `docker network create mynet`.
   - **Resource Limits**: Use `--cpus 2 --memory 4g` in `run` to cap usage.
   - **Environment Variables**: `-e VAR=value` to pass configs.
   - **Privileged Mode**: `--privileged` for full host access (use cautiously, security risk).
   - **Health Checks**: Add `--health-cmd` to monitor app health.
   - **Multi-Container Apps**: Use Docker Compose (below) for orchestrating services like your PySpark + Glue setup.
   - **Security**: Run as non-root with `--user`, scan images with `docker scan`, use secrets for sensitive data.

To become proficient: Practice with simple images like `hello-world` or `nginx`. Use the dashboard for visuals, but CLI for speed. Read docs with `docker --help` or online at docs.docker.com.

### Persistence and Mapping (Key for Your PySpark Jupyter + Glue Setup)

Containers are ephemeral by default—data is lost when deleted. To persist:

1. **Volumes** (Recommended for Docker-Managed Persistence):
   - Create: `docker volume create mydata`.
   - Mount in Run: `docker run -v mydata:/path/in/container ...`.
   - For your PySpark Jupyter: `docker run -d -p 8888:8888 -v pyspark-data:/home/jovyan/work --name pyspark-notebook jupyter/pyspark-notebook`.
     - This maps host volume `pyspark-data` to the notebook's work dir. Notebooks and data persist even if container is removed.
   - View/Manage: In Dashboard > Volumes tab. Inspect or prune unused ones.

2. **Bind Mounts** (Map Host Directories):
   - `docker run -v /host/path:/container/path ...`.
   - Example: `docker run -d -p 8888:8888 -v /Users/yourname/projects:/home/jovyan/work --name pyspark-notebook jupyter/pyspark-notebook`.
     - Maps your local folder to the container. Changes sync in real-time.
   - Great for development, but be careful with permissions (use `--user` if needed).

For AWS Glue integration in PySpark: Pull an image like `amazon/aws-glue-libs` or customize with a Dockerfile. Mount volumes for Glue scripts/data. Example run: Add `-e AWS_ACCESS_KEY_ID=yourkey` for AWS creds (better to use secrets or mounted configs for security).

### Docker Compose for Multi-Container Setups

If your PySpark + Glue involves multiple services (e.g., Spark master/worker, Jupyter), use Compose.

1. Create `docker-compose.yml`:
   ```
   version: '3'
   services:
     jupyter:
       image: jupyter/pyspark-notebook
       ports:
         - "8888:8888"
       volumes:
         - ./notebooks:/home/jovyan/work
     glue:
       image: amazon/aws-glue-libs
       volumes:
         - ./data:/data
   ```

2. Run: `docker-compose up -d` in the dir with the YML.
3. Manage: `docker-compose ps`, `down`, etc.
Compose is built into Docker Desktop—enable in settings if needed.

### Making the Most of Plugins/Extensions

In the Extensions menu (your "MCP"):
- Install "Docker Scout" for vulnerability scanning.
- "Resource Saver" for optimizing usage.
- "Jupyter" extensions if available for direct integration.
- Browse Marketplace for PySpark/Spark-specific tools (e.g., for monitoring clusters).
Extensions are community/plugin-driven—install only trusted ones.

### Path to Proficiency

- **Daily Practice**: Start/stop containers daily. Build a custom image with a Dockerfile (e.g., FROM jupyter/pyspark-notebook, ADD your scripts).
- **Resources**: 
  - Official Docs: docs.docker.com/get-started
  - Tutorials: Docker's "Learn" section in app.
  - Books: "Docker in Action" or free YouTube series.
- **Troubleshooting**: Check logs, use `docker inspect`, or forums like Stack Overflow.
- For your setup: Test persistence by saving a notebook, stopping the container, and restarting—data should remain.

If you hit issues with your PySpark/Glue container, share error logs or specifics for targeted help!